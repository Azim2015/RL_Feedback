import numpy as np
import torch
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
#from ray.rllib.agents.ppo import PPOTrainer
from ray.rllib.algorithms.ppo import PPO

from ray.tune.registry import register_env

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")



sbert_model_name = "sentence-transformers/all-MiniLM-L6-v2"
#sbert_model_name = AutoModelForSequenceClassification.from_pretrained(sbert_model_name).to(device)

tokenizer = AutoTokenizer.from_pretrained(sbert_model_name)
model = AutoModelForSequenceClassification.from_pretrained(sbert_model_name)


class PromptSelectionEnvWithFeedback:
    def __init__(self, config):
        self.text = config["text"]
        self.prompts = config["prompts"]
        self.max_steps = len(self.prompts)  # Max steps equal to number of prompts
        self.current_step = 0
        self.selected_prompts = []
        self.done = False
        self.rag_pipeline = pipeline("text2text-generation", model="facebook/rag-token-nq")

    def reset(self):
        """Resets the environment at the beginning of an episode."""
        self.current_step = 0
        self.selected_prompts = []
        self.done = False
        return {"text": self.text, "current_step": self.current_step}

    def step(self, action):
        """Executes a single step in the environment."""
        if self.done:
            raise ValueError("Step called on an already done environment.")
        
        
        self.selected_prompts.append(self.prompts[action])
        self.current_step += 1
        
        
        self.done = self.current_step >= self.max_steps
        
        
        initial_reward = self.evaluate_similarity(self.text, self.selected_prompts)
        
        
        rag_score = self.rag_feedback(self.selected_prompts, self.text)
        adjusted_reward = initial_reward * rag_score
        
        return {"text": self.text, "current_step": self.current_step}, adjusted_reward, self.done, {}

    def evaluate_similarity(self, text, selected_prompts):
        """Calculates SBERT-based similarity as initial reward."""
        embeddings_text = model(**tokenizer(text, return_tensors="pt", padding=True, truncation=True)).logits
        embeddings_prompts = model(
            **tokenizer(selected_prompts, return_tensors="pt", padding=True, truncation=True) #.to(device)
        ).logits
        similarity = torch.cosine_similarity(embeddings_text, embeddings_prompts, dim=1).mean()
        return similarity.item()

    def rag_feedback(self, selected_prompts, text):
        """Uses RAG model to provide feedback on selected prompts."""
        input_text = f"Evaluate: {text}\nSelected Prompts: {' '.join(selected_prompts)}"
        evaluation = self.rag_pipeline(input_text)[0]["generated_text"]
        score = self.evaluate_feedback(evaluation)
        return score

    def evaluate_feedback(self, evaluation):
        """Converts RAG evaluation text into a numeric score."""
        if "highly relevant" in evaluation.lower():
            return 1.5  # Higher reward multiplier
        elif "relevant" in evaluation.lower():
            return 1.0
        return 0.5  # Lower reward multiplier


def env_with_feedback_creator(config):
    return PromptSelectionEnvWithFeedback(config)

register_env("prompt_selection_env_with_feedback", env_with_feedback_creator)


config_with_feedback = {
    "env": "prompt_selection_env_with_feedback",  # Registered environment name
    "env_config": {
        "text": "The image depicts a man seated in the driver’s seat of a car, actively engaging in distracted behavior. He is holding a smartphone to his ear with his left hand while simultaneously holding a coffee cup in his right hand. The man appears to be conversing on the phone, with no visible focus on driving. He is dressed casually in a plaid shirt and khaki pants. The vehicle’s interior includes a visible steering wheel, dashboard, and hanging car keys. The situation highlights unsafe driving practices, such as multitasking behind the wheel, which can lead to accidents or other road safety hazards.",  # Input text for the environment

        "prompts": ["What is it about?", "Is there any vehicle in the scenario?", "How many people or pedestrian are there?"],  # List of prompts
    },
    "framework": "torch",  # Use PyTorch
    "num_workers": 1,  # Number of parallel workers
    "train_batch_size": 200,  # Training batch size
    "sgd_minibatch_size": 50,  # Minibatch size for SGD
}


#trainer = PPOTrainer(config=config_with_feedback)
trainer = PPO(config=config_with_feedback)

for i in range(10):  # Train for 10 iterations
    print(i)
    result = trainer.train()
    print(f"Iteration {i}, reward: {result['episode_reward_mean']}")
